"""
In parts based on https://github.com/CompVis/latent-diffusion/blob/main/scripts/txt2img.py

This module implements the main logic of our multi-stage blended diffusion approach, mainly in the BLDGenerator class.
@:author Johannes Ackermann
"""

import copy
from functools import partial
import logging
import time
import os
from typing import List, Optional, Tuple


import numpy as np
import torch
from einops import rearrange
from omegaconf import OmegaConf
from PIL import Image
from pytorch_lightning import seed_everything

from basicsr.archs.rrdbnet_arch import RRDBNet
import clip

from msbd.BLDSampler import BlendedDiffusionSampler
from msbd.msbd_utils import get_alpha_masks, tensor_to_pil, get_result_grid, display_or_save, sharpen
from ldm.models.diffusion.ddpm import LatentDiffusion
from ldm.util import instantiate_from_config
from realesrgan import RealESRGANer

logging.addLevelName(logging.WARNING, "\033[1;31m%s\033[1;0m" %
                     logging.getLevelName(logging.WARNING))
logging.addLevelName(logging.ERROR, "\033[1;41m%s\033[1;0m" % logging.getLevelName(logging.ERROR))

logger = logging.getLogger()
logging.basicConfig()
logging.getLogger().setLevel(logging.INFO)


def load_model_from_config(config, ckpt, verbose=False):
    print(f"Loading model from {ckpt}")
    pl_sd = torch.load(ckpt, map_location="cpu")
    sd = pl_sd["state_dict"]
    model = instantiate_from_config(config.model)
    m, u = model.load_state_dict(sd, strict=False)
    if len(m) > 0 and verbose:
        print("missing keys:")
        print(m)
    if len(u) > 0 and verbose:
        print("unexpected keys:")
        print(u)

    model.cuda()
    # logger.warn('Not setting the model to eval!!!')
    model.eval()
    return model


def get_model(config, device, use_fp16, stable_diffusion=False) -> LatentDiffusion:
    if stable_diffusion:
        model = load_model_from_config(
            config, "models/ldm/stable-diffusion-v1/model.ckpt")
    else:
        base_model_fp = "models/ldm/text2img-large/model.ckpt"
        model = load_model_from_config(config, base_model_fp)
    if use_fp16:
        model.half()
    return model.to(device)


    
class MSBDGenerator(object):
    def __init__(
        self,
        use_fp16: bool = True,
        stable_diffusion: bool = True,
        max_edgelen: int = 12*64,
        first_stage_batchsize: int = 1,
    ) -> None:
        """This implements the whole Multi-Scale Blended Diffusion approach.
        Most usage should simply call the `multi_scale_generation` member function.

        Args:
            use_fp16 (bool, optional): Whether to run the model at FP16, usually recommended. Defaults to True.
            stable_diffusion (bool, optional): If True uses stable diffusion, if False uses the default txt2img LDM. Defaults to True.
            max_edgelen (int, optional): Edge length of maximum square area that can be generated by used graphics card. Defaults to 12*64.
            first_stage_batchsize (int, optional): Batch size for first stage of multi-stage blended diffusion. Defaults to 1.
        """

        self.device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

        logger.info(f'Loading model...')
        if stable_diffusion:
            config = OmegaConf.load("configs/stable-diffusion/v1-inference.yaml") 
        else:
            config = OmegaConf.load("configs/latent-diffusion/txt2img-1p4B-eval.yaml")
        self.model = get_model(config, self.device, use_fp16, stable_diffusion=stable_diffusion)
        logger.info(f'Finished loading model...')
        self.sampler = BlendedDiffusionSampler(self.model)
        self.last_decoderopt_dict = None
        self.default_size = 512
        self.is_stable_diffusion = stable_diffusion
        self.max_edgelen = max_edgelen
        self.first_stage_batchsize = first_stage_batchsize

    def multi_scale_generation(
        self, 
        pil_img: Image, 
        pil_mask: Image,
        prompt: str, 
        margin: float = 1.0, 
        decoder_optimization: bool = False,
        start_timestep: float = 1.0,
        upscaling_start_step: float = 0.35,
        grid_upscaling_start_step: float = 0.25,
        repaint_steps: int = 0,
        tight_stitch: bool = True,
        blended_upscale: bool = True,  # this means that during upscaling we're using blended diffusion such that the model gets the (noised) target-resolution reference image while upscaling only the masked part
        conditional_upscale: bool = True,  # use prompt in upscaling
        ddim_steps: int = 50,
        grid_overlap: int = 128,  # overlap for the grid-like upscaling
        straight_to_grid: bool = False,  # skip any cascaded stages, go straight from first stage to grid-like upscaling
        decoder_opt_it: int = 100,
        clip_reranking: bool = True,
        seed: int = -1,
        log_folder: str = 'output/',
        upscaling_mode: str = 'esrgan',  # 'esrgan' or 'bilinear'
        lowpass_reference: str = 'matching',
        first_stage_size: Optional[int] = None,
        debug_outputs: bool = False
    ) -> Image:
        """Peform our full multi-scale generation approach.
        Starts by running the diffusion model at its base-resolution, in a square around the mask.
        For following upsampling stages the area is no longer square but fitted in a rectangle to the mask.

        Args:
            pil_img (Image): input image as PIL image, should be mode 'RGB'
            pil_mask (Image): input mask as PIL image, should be mode 'L'. The region to be edited should be nonzero, the rest 0
            prompt (str): prompt to edited the masked region with
            margin (float, optional): By default, we crop to a region around the masked area. However, when `margin`!=1.0 is specified we crop to an area that has the
                length and width of the mask multiplied by `margin`. Defaults to 1.0.
            decoder_optimization (bool, optional): Whether to perform decoder optimization or not. Improves blending with the background, but takes a significant
                amount of time for high resolutions.. Defaults to False.
            start_timestep (float, optional): SDEdit-like start time-step for the first-stage, also referred to as img-to-img in the stable-diffusion repo. Defaults to 1.0.
            upscaling_start_step (float, optional): SDEdit-like start time-step for the upscaling models. Defaults to 0.35.
            grid_upscaling_start_step (float, optional): Upscaling start-timestep for the final grid stage. This should probably be lower than in the previous stages as there is less
                context available for the model in each segment. Defaults to 0.25.
            repaint_steps (int, optional): Number of repaint steps to perform in the first stage. Defaults to 0.
            tight_stitch (bool, optional): if True, we crop to only the masked region when inserting the edited region into the source image. if False, the full rectangular region is inserted.
            blended_upscale (bool, optional): if True, we use blended diffusion in the upscaling by passing the input image as reference. Defaults to True.
            conditional_upscale (bool, optional): If true, uses the text prompt in the upscaling. Defaults to True.
            ddim_steps (int, optional):  Number of diffusion steps to perform in the first stage. Doubled in upscaling stages. Defaults to 50.
            grid_overlap (int, optional):  overlap of different regions in the grid-like upscaling. Defaults to 128.
            straight_to_grid (bool, optional): skip any intermediate stages, we run the model once at its base resolution and then immediately do the upscaling in a grid-like way.. Defaults to False.
            decoder_opt_it (int, optional): Iterations for decoder optimization. Defaults to 100.
            clip_reranking (bool, optional): at the base resolution of the diffusion model we generate a batch of 5 images and pick the "best" one by clip similarity to the given prompt. Defaults to True.
            seed (int, optional): Seed for everything, if -1 we pick a random seed. Defaults to -1.
            log_folder (str, optional): path to a folder which intermediate and final images will be logged to.
            upscaling_mode (str, optional): upscaling mode used in the upscaling steps before the diffusion. Can be 'esrgan' or any mode supported 
                by torch.functional.interpolate, i.e. 'bilinear', 'bicubic'. Defaults to 'esrgan.
            lowpass_reference (str, optional): whether to low-pass filter the reference image in the blended upscaling. Can be 'matching' to match the edit image's
                effective resolution, 'half' to be halfed, or 'no' to not do any lowpass filtering
        Returns:
            Image: Full image including the edited region.
        """    

        fp_gen = lambda x: os.path.join(log_folder, prompt.replace(" ", "_").replace(".","-") + f'{x:03d}')
        counter = 0
        out_fp = fp_gen(counter)
        while os.path.exists(out_fp):
            counter += 1
            out_fp = fp_gen(counter)
        os.makedirs(out_fp)

        logger.info(f'starting multi_scale_generation for prompt \"{prompt}\"')

        assert margin >= 1.0
        
        ##### get mask bbox and crop for first stage
        mask_height = np.max(np.nonzero(np.array(pil_mask))[0]) - np.min(
            np.nonzero(np.array(pil_mask))[0]
        )
        mask_width = np.max(np.nonzero(np.array(pil_mask))[1]) - np.min(
            np.nonzero(np.array(pil_mask))[1]
        )
        print("mask size:", mask_height, mask_width)
        if first_stage_size is not None:
            min_diff_size = first_stage_size
        else:
            min_diff_size = self.default_size
        diff_height = max(
            [
                min(int(np.ceil(mask_height / 64 * margin) * 64), pil_img.size[0] // 64 * 64),
                min_diff_size,
            ]
        )
        diff_width = max(
            [
                min(int(np.ceil(mask_width / 64 * margin) * 64), pil_img.size[1] // 64 * 64),
                min_diff_size
             ]
        )

        print("diffusion size:", diff_width, diff_height)

        image_full, mask_full = MSBDGenerator.image_mask_from_pil(pil_img, pil_mask)
        image_crop_square, mask_crop_square, _, crop_x, crop_y = MSBDGenerator.crop_image_and_mask(
            image_full, mask_full, crop_width=max(diff_height, diff_width), crop_height=max(diff_height, diff_width)
        )

        #########
        #  first stage
        #########
        crop_image_ds, crop_mask_pixel_ds, crop_mask_latent_ds = self.interpolate_img_mask(image_crop_square, mask_crop_square, min_diff_size, min_diff_size)
        sample_latents, image_samples = self.run_first_stage(
            source_img=crop_image_ds,
            mask=crop_mask_pixel_ds,
            mask_ds=crop_mask_latent_ds,
            prompt=prompt,
            seed=seed,
            ddim_steps=ddim_steps,
            repaint_steps=repaint_steps,
            repaint_jump=0,
            dilate_mask=False,
            start_timestep=start_timestep,
            n_samples=self.first_stage_batchsize,
            scale = 7.5 if self.is_stable_diffusion else 5.0,
        )
        if debug_outputs:
            pil_grid = self.save_results_and_grid(
                image_samples, crop_image_ds, crop_mask_pixel_ds, prompt, "outputs/blended_samples"
            )
            display_or_save(pil_grid, out_fp, 'first_stage_result')

        if clip_reranking and self.first_stage_batchsize > 1:
            clip_start = time.time()
            clip_model, preprocess = clip.load("ViT-L/14", device='cpu')
            probs = []
            
            for sample in image_samples:
                image = preprocess(tensor_to_pil(sample[None] * 2.0 - 1.0)).unsqueeze(0)
                text = clip.tokenize(prompt)

                with torch.no_grad():                    
                    logits_per_image, _ = clip_model(image, text)
                probs.append(logits_per_image.detach().numpy()[0][0])
            best_fit = np.argmax(probs)
            
            logger.info(f'clip took {time.time() - clip_start:.2f}, probabilities: {probs}')
            image_samples = image_samples[best_fit:best_fit + 1]
        else:
            image_samples = image_samples[0:1]
        if debug_outputs:
            display_or_save(tensor_to_pil(2 * image_samples - 1.0), out_fp, 'first_stage_selected')


        if decoder_optimization:
            image_samples = self.decoder_optimization(
                input_sample=image_samples.float() * 2.0 - 1.0,
                reference_img = crop_image_ds.to(sample_latents.device),
                mask = crop_mask_pixel_ds.to(sample_latents.device),
                n_decoder_opt_it=decoder_opt_it,
                background_preserve_weight=100.0,
                learn_rate=1e-4
            )
            if debug_outputs:
                pil_grid = self.save_results_and_grid(
                    image_samples, image_crop_square, mask_crop_square, prompt, "outputs/blended_samples"
                )
                display_or_save(pil_grid, out_fp, 'first_stage_dec_optimized')

        # generate exponential upscaling schedule
        max_scaling_factor = max(diff_width, diff_height) / min_diff_size
        stage_scaling_factors = np.power(2, np.arange(0.0, np.log2(max_scaling_factor), 0.5))
        upsampling_stages = stage_scaling_factors * min_diff_size
        upsampling_stages = np.ceil(upsampling_stages / 64).astype(int) * 64
        upsampling_stages = upsampling_stages[upsampling_stages <= self.max_edgelen]
        upsampling_stages = list(upsampling_stages)[1:] + [max(diff_width, diff_height)]
        print(f'upsampling stages: {upsampling_stages}')
        
        if straight_to_grid:
            upsampling_stages = [max(diff_width, diff_height)]

        #####
        # second stage till last stage
        #####
        for current_edgelen in upsampling_stages:  # the edgelength always refers to the longer edge here, and current=current target

            # calculate the height and width of the diffusion input
            if diff_width > diff_height:
                current_width = current_edgelen
                current_height = int(np.ceil((current_width * diff_height / diff_width) / 64) * 64)
                current_input_width = image_samples.shape[3]
                current_input_height = int(current_input_width * current_height / current_width)
            elif diff_height > diff_width:
                current_height = current_edgelen
                current_width = int(np.ceil((current_height * diff_width / diff_height) / 64) * 64)
                current_input_height = image_samples.shape[2]
                current_input_width = int(current_input_height * current_width / current_height)
            else:
                current_width = current_height = current_edgelen
                current_input_width = image_samples.shape[3]
                current_input_height = image_samples.shape[2]
            np.testing.assert_allclose(current_input_width / current_input_height, current_width / current_height, atol=0.05, rtol=0.05)
            
            # we may need to adjust the aspect ratio of the crop for this stage, so we reinsert the editted image into
            # the full image after bilinear upscaling and recrop around the mask with the new width and height
            current_zoom_ratio = current_edgelen / upsampling_stages[-1]
            crop_width_fullimg = int(current_width / current_zoom_ratio)
            crop_height_fullimg = int(current_height / current_zoom_ratio)
            edit_image_crop, reference_image_crop, mask_crop, crop_x, crop_y = \
                self.upscale_recrop_image(image_samples, image_full, mask_full, crop_x, crop_y, crop_width_fullimg, crop_height_fullimg)
            
            edit_image_crop_ds, _, _ = self.interpolate_img_mask(edit_image_crop, mask_crop, current_input_height, current_input_width )
            reference_image_crop_ds, crop_mask_pixel_ds, _ = self.interpolate_img_mask(reference_image_crop, mask_crop, current_height, current_width)
            
            if debug_outputs:
                display_or_save(tensor_to_pil(2 * edit_image_crop_ds - 1.0), out_fp, f'upscale{current_edgelen}in')
                # display_or_save(tensor_to_pil(reference_image_crop_ds), out_fp, f'upscale{current_edgelen}reference_in.png')
                # display_or_save(Image.fromarray(np.array(crop_mask_pixel_ds[0,0]).astype(np.uint8) * 255), out_fp, f'upscale{current_edgelen}mask_in.png')

            if current_edgelen <= self.max_edgelen:
                sample_latents, image_samples, _ = self.upscale(
                    edit_image_crop_ds.to(self.device) * 2.0 - 1.0,
                    target_imagesize=[current_height, current_width],
                    reference_image=reference_image_crop_ds.to(self.device) if blended_upscale else None,
                    mask=crop_mask_pixel_ds.to(self.device) if blended_upscale else None,
                    ddim_steps=2 * ddim_steps,
                    prompt=prompt if conditional_upscale else '',
                    start_timestep=upscaling_start_step,
                    # repaint_steps=repaint_steps,
                    # repaint_jump=5,
                    seed=seed,
                    out_fp=None,
                    esrgan_log_name=f'upscale{current_edgelen}esrgan',
                    interpolation=upscaling_mode,
                    lowpass_reference=lowpass_reference
                )
            else:
                sample_latents, image_samples = self.upscale_in_parts(
                    edit_image_crop_ds.to(self.device) * 2.0 - 1.0,
                    target_imagesize=[current_height, current_width],
                    reference_image=reference_image_crop_ds.to(self.device) if blended_upscale else None,
                    mask=crop_mask_pixel_ds.to(self.device) if blended_upscale else None,
                    ddim_steps=2 * ddim_steps,
                    prompt=prompt if conditional_upscale else '',
                    start_timestep=grid_upscaling_start_step,
                    # repaint_steps=repaint_steps,
                    overlap=grid_overlap,
                    # repaint_jump=5,
                    debug_outputs=False,
                    seed=seed,
                    out_fp = None,
                    interpolation=upscaling_mode,
                    lowpass_reference=lowpass_reference
                )
                # logger.warn('Skipping decoder optimization at final stage, hardcoded in the source!')
                # decoder_optimization = False

            if debug_outputs:
                pil_img = tensor_to_pil(2 * image_samples - 1.0)
                display_or_save(pil_img, out_fp, f'upscale{current_edgelen}out')
            # pil_img.save(f'upscale{current_edgelen}out.png')

            if decoder_optimization:
                image_samples = self.decoder_optimization(
                    input_sample=image_samples.float() * 2.0 - 1.0,
                    reference_img = reference_image_crop_ds.to(self.device),
                    mask = crop_mask_pixel_ds.to(self.device),
                    n_decoder_opt_it=decoder_opt_it,
                    background_preserve_weight=100.0,
                    learn_rate=1e-4
                )
            # pil_img = tensor_to_pil(2 * image_samples - 1.0)
            # display_or_save(pil_img, out_fp, f'{current_edgelen}_stage_out_dec_opt')

        ############
        # stitch result
        ###########
        tight_stitch_batch = []
        for x in image_samples:
            full_stitch = (image_full.clone() + 1.0) / 2.0
            if tight_stitch:  # replaces unmasked pixels in the editted region with the pixels of the original image. This works when blending is good, otherwise creates a visible border
                x = crop_mask_pixel_ds * x.cpu() + (1 - crop_mask_pixel_ds) * torch.clamp((reference_image_crop_ds + 1.0) /2.0, min= 0.0, max=1.0)
            full_stitch[0, :, crop_x[0] : crop_x[1], crop_y[0] : crop_y[1]] = x
            tight_stitch_batch.append(full_stitch)
        tight_stitch_batch = torch.vstack(tight_stitch_batch)

        # pil_grid_highres_tightstich = self.save_results_and_grid(
        #     tight_stitch_batch,
        #     image_full,
        #     mask_full,
        #     prompt,
        #     "outputs/blended_samples",
        #     suffix="_highres_tightstitch",
        # )
        # display_or_save(pil_grid_highres_tightstich, out_fp, 'final_editregion')
        if debug_outputs:
            display_or_save(tensor_to_pil(tight_stitch_batch * 2.0 - 1.0), out_fp, 'final_fullimg')
        return tensor_to_pil(tight_stitch_batch * 2.0 - 1.0) 

    def upscale_recrop_image(self, edited_image_segment, image_full, mask_full, prev_edit_x, prev_edit_y, new_width, new_height):
        """
        This function takes an edited lower resolution segment of a full image, upscales it bilinearly and reinserts it into the full image.
        The image and mask are then cropped again around the given mask with given height and width, and returned along with the new crop's
        coordinates.

        The reason that this function is necessary is that the image is edited/upscaled in multiple stages, where the aspect ratio may change
        from stage to stage, due to the requirement that each dimension be a mutliple of 64 pixels long.
        """
        edited_image_segment = torch.nn.functional.interpolate(
            edited_image_segment,
            size=[prev_edit_x[1] - prev_edit_x[0], prev_edit_y[1] - prev_edit_y[0]],
            mode="bilinear",
            align_corners=False,
            antialias=True,
        )
        first_stage_stitched = image_full.clone()
        first_stage_stitched[0, :, prev_edit_x[0] : prev_edit_x[1], prev_edit_y[0] : prev_edit_y[1]] = edited_image_segment

        sample_crop, mask_crop, _, crop_x, crop_y = MSBDGenerator.crop_image_and_mask(
            first_stage_stitched, mask_full, crop_width=new_width, crop_height=new_height)

        reference_crop, mask_crop, _, crop_x, crop_y = MSBDGenerator.crop_image_and_mask(  # this one is necessary as the reference in upsampling
            image_full, mask_full, crop_width=new_width, crop_height=new_height)
            
        return sample_crop, reference_crop, mask_crop, crop_x, crop_y,

    def interpolate_img_mask(self, image, mask, width, height) -> Tuple[torch.Tensor]:
        """
        Given image and mask and desired shape, this function interpolates the image and mask to the desired  shape and returns both, 
        as well as a version of the mask further downsampled x8 to be used in the latent space during sampling.
        """
        image_ds = torch.nn.functional.interpolate(
            image, size=[width, height], mode="bilinear", align_corners=False, antialias=True,
        )
        mask_pixel_ds = torch.nn.functional.interpolate(
            mask, size=[width, height], mode="bilinear", align_corners=False
        )
        mask_pixel_ds[mask_pixel_ds > 0.0] = 1.0
        mask_latent_ds = torch.nn.functional.interpolate(
            mask,
            size=[width // 8, height // 8],
            mode="bilinear",
            align_corners=False,
        )
        mask_latent_ds[mask_latent_ds > 0.0] = 1.0
        return image_ds, mask_pixel_ds, mask_latent_ds       

    def run_first_stage(
        self,
        source_img: torch.Tensor,
        mask: torch.Tensor,
        mask_ds: torch.Tensor,
        prompt: str,
        seed: int = -1,
        ddim_eta: float = 1.0,
        scale: float = 5.0,
        n_samples: int = 5,
        ddim_steps: int = 50,
        dilate_mask: bool = False,
        repaint_steps: int = 0,
        repaint_jump: int = 0,
        start_timestep: float = 1.0,
    ) -> Tuple[torch.Tensor]:
        """Representing the first stage of multi-stage blended diffusion, this function runs blended diffusion to edit the `source_img` given `mask` to match the prompt.

        Args:
            source_img (torch.Tensor): Input image in pixel space, will be encoded to latent internally.
            mask (torch.Tensor): editing mask in pixel space
            mask_ds (torch.Tensor): editing mask downscaled to latent space
            prompt (str): prompt desribing the new content of the masked region
            seed (int, optional): Seed for the generation, set to -1 for random. Defaults to -1.
            ddim_eta (float, optional): Whether to run stochastic decoding, 0.0 for deterministic. Defaults to 1.0.
            scale (float, optional): Scale for the classifier-free guidance. Defaults to 5.0.
            n_samples (int, optional): batch size for the first stage. Defaults to 5.
            ddim_steps (int, optional): Number of ddim steps for the reverse process. Defaults to 50.
            dilate_mask (bool, optional): Whether to use mask dilation, increasing the size of the mask during early diffusion steps. Defaults to False.
            repaint_steps (int, optional): Repaint steps, i.e. number of repetitions of each repaint jump. Defaults to 0.
            repaint_jump (int, optional): repaint jump size, if 0 simply repeats the current diffusion step. Defaults to 0.
            start_timestep (float, optional): Start time-step for the blended diffusion, as in SDEdit can be used for image2image editing. Defaults to 1.0.

        Returns:
            Tuple[torch.Tensor]: output 0: edited image latents; output 1: edited decoded images 
        """
        if source_img.min() >= 0.0:
            logger.warning(f'This function should receive an image scaled to [-1.0, 1.0], not [0.0, 1.0]')

        if repaint_steps and ddim_eta == 0.0:
            logger.warn(
                'Repaint with deterministic decoding decreases sample diversity (and also quality seemingly).')

        if seed == -1:
            seed = np.random.randint(int(1e6))
        logger.info(f'Using seed {seed}')

        height = source_img.shape[2]
        width = source_img.shape[3]

        seed_everything(seed)


        mask = mask.to(self.device)
        source_img = source_img.to(self.device)

        with torch.autocast(device_type='cuda'), torch.no_grad(), self.model.ema_scope():
            uncond_conditioning = None
            if scale != 1.0:
                uncond_conditioning = self.model.get_learned_conditioning(n_samples * [""])
            conditioning = self.model.get_learned_conditioning(n_samples * [prompt])
            shape = [4, height//8, width//8]

            source_img_encoded = self.model.scale_factor * \
                self.model.encode_first_stage(source_img).mean
            samples_ddim, intermediates = self.sampler.blended_diffusion_sampling(
                source_img=source_img_encoded,
                mask=mask_ds,
                num_ddim_steps=ddim_steps,
                conditioning=conditioning,
                batch_size=n_samples,
                shape=shape,
                verbose=False,
                unconditional_guidance_scale=scale,
                unconditional_conditioning=uncond_conditioning,
                eta=ddim_eta,
                dilate_mask=dilate_mask,
                repaint_steps=repaint_steps,
                repaint_jump=repaint_jump,
                start_timestep=start_timestep,
            )
            x_samples_ddim = self.model.decode_first_stage(samples_ddim)
            x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0, min=0.0, max=1.0)


        return samples_ddim, x_samples_ddim

    def save_results_and_grid(
        self, 
        sample_imgs: torch.Tensor, 
        source_img: torch.Tensor, 
        mask: torch.Tensor, 
        prompt: str, 
        outdir: str, 
        suffix: Optional[str] = None,
        save_samples: bool = False,
    ) -> Image:
        """
        Takes a tensor containing multiple sample _images_ in the format [batch, channel, h, w] and 
        saves them to `outdir`/samples, and a grid visualizing the mask and samples to `outdir`.
        
        Returns the grid as a pil_img
        """        
        if suffix is None:
            suffix = ''

        if save_samples:
            os.makedirs(outdir, exist_ok=True)
            sample_path = os.path.join(outdir, "samples")
            os.makedirs(sample_path, exist_ok=True)
            file_count = len(os.listdir(sample_path))
            for x_sample in sample_imgs:
                x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')
                sample_fp = os.path.join(sample_path, f"{file_count:04}.png")
                Image.fromarray(x_sample.astype(np.uint8)).save(sample_fp)
                logging.info(f'saved sample to {sample_fp}')
                file_count += 1

        pil_grid = get_result_grid(source_img, [sample_imgs], mask)

        if save_samples:
            if not prompt:
                prompt = 'empty_prompt'

            def fp_name(count): 
                return os.path.join(outdir, f'{prompt.replace(" ", "-")}{count:02}{suffix}.png')
            counter = 0
            out_fp = fp_name(counter)
            while os.path.exists(out_fp):
                counter += 1
                out_fp = fp_name(counter)
            pil_grid.save(out_fp)
            print(f"Your samples are ready and waiting four you here: \n{out_fp} \nEnjoy.")

        return pil_grid
    
    def determine_upsampling_split(
        self, 
        target_image_size: tuple, 
        max_edgelen: int, 
        overlap: int = 64,
        enforce_min_size: bool = True,
    ) -> Tuple[List[np.ndarray]]:
        """
        Returns pixel crop coordinates that cover an area of `target_image_size`
        in squares of size `maximum_edgelen` or less, but at least of size self.default_size.
        
        `overlap` determines how far each grid reaches into the next grid. i.e, if we set overlap=64, 
        `target_image_size` to 1536x1536 and maximum_diffusion_size to 640, the grid will be
        [0:640, 576:1216,...] and the first segment will be used to fill in 0:576, the second segment will
        fill in 640:... and 576:640 will be interpolated between the two.
        
        TODO: This could be changed to ensure that every region has a signifcant amount of masked area in it.
        Sometimes the resulting regions will be mostly unmasked regions with only a tiny bit of masked area in it,
        which sometimes causes bad results. They could also not include any masked pixels if the margin is very large,
        wasting significant compute.
        """
        if target_image_size[0] < max_edgelen and target_image_size[1] < max_edgelen:
            return [np.array([0, target_image_size[0]])], [np.array([0, target_image_size[1]])]

        crops_x = []
        crops_y = []
        x_start = 0
        while x_start < target_image_size[0]:
            y_start = 0
            while y_start < target_image_size[1]:
                x_end = min(target_image_size[0], x_start + max_edgelen)
                y_end = min(target_image_size[1], y_start + max_edgelen)
                if enforce_min_size and x_end - x_start < self.default_size:
                    x_start = x_end - self.default_size
                if enforce_min_size and y_end - y_start < self.default_size:
                    y_start = y_end - self.default_size
                crops_x.append(np.array([x_start, x_end]))
                crops_y.append(np.array([y_start, y_end]))
                y_start += max_edgelen - overlap
            x_start += max_edgelen - overlap        
        
        
        return crops_x, crops_y

    @torch.no_grad()
    def upscale_in_parts(
        self,
        input_image: torch.Tensor,
        prompt: str,
        reference_image: Optional[torch.Tensor] = None,
        mask: Optional[torch.Tensor] = None,
        upscale_factor: float = None,
        target_imagesize: Tuple[int] = None,
        ddim_steps: int = 50,
        start_timestep: float = 0.3,
        repaint_steps: int = 0,
        repaint_jump: int = 0,
        maximum_edgelength: int = None,
        overlap: int = 256,
        debug_outputs: bool = False,
        seed: int = -1,
        out_fp: str = None,
        interpolation: str = 'esrgan',
        lowpass_reference: str = 'matching',
    ) -> torch.Tensor:
        """
            Upscales an image in parts with overlap.
            Also works for a single part, and shares the same required arguments, 
            so it can be used as a drop-in replacement for self.upscale, IF the latent output is not needed.
        """
        if maximum_edgelength is None:
            maximum_edgelength = self.max_edgelen
        input_imagesize = np.array(input_image.size()[2:])

        if not (reference_image is None) == (mask is None):
            raise ValueError('Specify both mask and reference_image or neither.')

        if input_image.min() >= 0.0:
            logger.warning(
                f'This function should receive images scaled to [-1.0, 1.0], not [0.0, 1.0]')
        if not target_imagesize:
            target_imagesize = np.ceil(input_imagesize * upscale_factor).astype(int)
        else:
            assert target_imagesize[0] % 64 == target_imagesize[1] % 64 == 0
            target_imagesize = np.array(target_imagesize).astype(int)

        # split image into parts which after upscaling will be the right size
        crops_x, crops_y = self.determine_upsampling_split(target_imagesize, maximum_edgelength, overlap=overlap)  

        logger.info(f'Running upscaling to {target_imagesize} in {len(crops_x)} segments of [f{maximum_edgelength} x {maximum_edgelength}].')
        upscaled_segments = []
        for idx_segment, (crop_x, crop_y) in enumerate(zip(crops_x, crops_y)):
            logger.info(f'Upscaling segment {idx_segment}/{len(crops_x)}.')
            upscale_factor = target_imagesize[0] / input_imagesize[0]
            crop_x_lowres = (crop_x / upscale_factor).astype(int)
            crop_y_lowres = (crop_y / upscale_factor).astype(int)
            
            image_segment_lowres = input_image[:, :, crop_x_lowres[0]:crop_x_lowres[1], crop_y_lowres[0]:crop_y_lowres[1]]
            if mask is not None: 
                mask_segment = mask[:, :, crop_x[0]:crop_x[1], crop_y[0]:crop_y[1]]
                reference_segment = reference_image[:, :, crop_x[0]:crop_x[1], crop_y[0]:crop_y[1]]
            else:
                mask_segment = None
                reference_segment = None
            
            if debug_outputs:
                pil_img = tensor_to_pil(image_segment_lowres)
                display_or_save(pil_img, out_fp, f'upscale-segment-{idx_segment}in')
                # pil_img.save(f'upscale-segment-{idx_segment}in.png')
                # if reference_segment is not None:
                #     display_or_save(tensor_to_pil(reference_segment), out_fp, f'upscale-segment-{idx_segment}-reference_in.png')
                #     display_or_save(Image.fromarray(np.array(mask_segment.cpu()[0,0]).astype(np.uint8) * 255), out_fp, f'upscalesegment-{idx_segment}-mask_in.png')

            _, image_segment_upscaled, _ = self.upscale(
                input_images=image_segment_lowres,
                prompt=prompt,
                reference_image=reference_segment,
                mask=mask_segment,
                target_imagesize=[crop_x[1] - crop_x[0], crop_y[1] - crop_y[0]],
                ddim_steps=ddim_steps,
                start_timestep=start_timestep,
                repaint_steps=repaint_steps,
                repaint_jump=repaint_jump,
                seed=seed,
                out_fp = out_fp,
                esrgan_log_name= f'upscale-segment-{idx_segment}esrgan',
                interpolation=interpolation,
                lowpass_reference=lowpass_reference
            )
            if debug_outputs:
                pil_img = tensor_to_pil(2 * image_segment_upscaled - 1.0)
                display_or_save(pil_img, out_fp, f'upscale-segment-{idx_segment}out')
                # pil_img.save(f'upscale-segment-{idx_segment}out.png')

            
            upscaled_segments.append(image_segment_upscaled)

        # reassemble image segments
        if overlap:
            result_image = self.alpha_compose_grid(input_image, upscaled_segments, crops_x, crops_y, target_imagesize, overlap)
        else:
            result_image = torch.ones([1, 3, *target_imagesize], dtype=input_image.dtype, device=input_image.device)
            for crop_x, crop_y, segment in zip(crops_x, crops_y, upscaled_segments):
                result_image[0, :, crop_x[0]:crop_x[1], crop_y[0]:crop_y[1]] = segment
        return None, result_image

    def alpha_compose_grid(
        self,
        source_img: torch.Tensor,
        upscaled_segments: List[torch.Tensor], 
        crops_x: List[Tuple[int]], 
        crops_y: List[Tuple[int]], 
        target_imagesize: Tuple[int],
        overlap: int,
    ) -> torch.Tensor:
        """Alpha composites a set of `upscaled_segments` over the given `source_img`.

        Args:
            source_img (torch.Tensor): base image over which the other segments are blended. Could be removed.
            upscaled_segments (List[torch.Tensor]): segments that will be composed
            crops_x (List[Tuple[int]]): list of start and end of each segment on x-axis
            crops_y (List[Tuple[int]]): list of start and end of each segment on y-axis
            target_imagesize (Tuple[int]): total size of the output
            overlap (int): overlap amount between segments

        Returns:
            torch.Tensor: composited image
        """
        pil_segments = [tensor_to_pil(seg * 2.0 - 1.0).convert('RGBA') for seg in upscaled_segments]
        combined_img = tensor_to_pil(source_img).resize(target_imagesize[::-1]).convert('RGBA')
            
        alpha_masks = get_alpha_masks(crops_x, crops_y, target_imagesize, overlap)
        for crop_x, crop_y, segment, alpha_mask in zip(crops_x, crops_y, pil_segments, alpha_masks):
            alpha_pil = Image.fromarray((alpha_mask * 255).astype(np.uint8))
            segment.putalpha(alpha_pil)
            combined_img.alpha_composite(segment, (crop_y[0],crop_x[0]))

        image = np.array(combined_img.convert('RGB')).astype(np.float32)/255.0
        image = image[None].transpose(0, 3, 1, 2)
        image = torch.from_numpy(image)
        return image.to(source_img.dtype).to(source_img.device)

    def realesrgan_upsample(self, img: torch.Tensor, target_imagesize: Tuple[int]) -> torch.Tensor:
        """
        Upscales 4x with RealESRGan and then downscales again to the `target_imagesize`.
        """
        img_pil = tensor_to_pil(img)  
        img = np.array(img_pil)
        # yes that's a waste of time to go from tensor to pil to array, but it prevents errors in the conversion
        sr_model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64,
                           num_block=23, num_grow_ch=32, scale=4)
        netscale = 4
        upsampler = RealESRGANer(
            scale=netscale,
            model_path='RealESRGAN_x4plus.pth',
            model=sr_model,
            tile=0,
            tile_pad=10,
            pre_pad=0,
            half=True,
            gpu_id=None
        )
        output, _ = upsampler.enhance(img, outscale=4)

        output = output.astype(np.float32) / 255.0
        output = output[None].transpose(0, 3, 1, 2)
        output = torch.from_numpy(output * 2.0 - 1.0)
        output_resized = torch.nn.functional.interpolate(
            output,
            size=[target_imagesize[0], target_imagesize[1]],
            mode='bilinear',
            antialias=True,
        )
        return output_resized.to(self.model.dtype).to(self.device)


    @torch.no_grad()
    def upscale(
        self, 
        input_images: torch.Tensor,
        prompt: str,
        reference_image: Optional[torch.Tensor] = None,
        mask: Optional[torch.Tensor] = None,
        upscale_factor: float = None,
        target_imagesize: Tuple[int] = None,
        ddim_steps: int = 50, 
        start_timestep: float = 0.3,
        repaint_steps: int = 0,
        repaint_jump: int = 0,
        interpolation: str = 'esrgan',
        ddim_eta: float = 1.0,
        seed: int = -1,
        lowpass_reference: str = 'matching',  # 'matching' 'half' or 'no'
        out_fp: str = None,
        esrgan_log_name: str = None,
        dilate_mask: bool = False,  #mainly for debugging
    ) -> torch.Tensor:
        """
        Upscales samples by first: 
         1. upscaling them using the interpolation specified in `interpolation`, either 'esrgan' or some mode accepted by 
         `torch.nn.functional.interpolate`.
         2. performs SDEdit like blended diffusion, addign noise till timestep `start_timestep` and then running the backward process.
        
        The backward process can be either text conditional when giving a prompt or unconditional when given an empty string as prompt.
        If a reference image and mask are given, we use blended diffusion for the upscaling, after low-pass filtering the reference
        if so specified in `lowpass_reference`.

        Takes an image as input as torch.Tensor in pixel space, not as latent.
        Specify either target_imagesize or an upscale factor.
        
        The mask should be at the target resolution.
        """

        if seed == -1:
            seed = np.random.randint(int(1e6))
        logger.info(f'Using seed {seed} in upscaling')
        seed_everything(seed)

        if input_images.min() >= 0.0:
            logger.warning(f'This function should receive images scaled to [-1.0, 1.0], not [0.0, 1.0]')

        if target_imagesize is None and upscale_factor is None or upscale_factor and target_imagesize:
            raise ValueError('Specify either upscale_factor or target_latentsize.')

        if target_imagesize == list(input_images.size()[2:]):
            logger.warn('Upscale in parts called with input size already being target imagesize!')

        if not (reference_image is None) == (mask is None):
            raise ValueError('Specify both mask and reference_image or neither.')

        n_samples = input_images.shape[0]
        image_size_in = np.array(input_images.shape[2:])
        assert target_imagesize[0] % 64 == target_imagesize[1] % 64 == 0, f'Input image size not divisible by 64: {image_size_in}'
        if not target_imagesize:
            target_imagesize = np.ceil(image_size_in * upscale_factor).astype(int)
        else:
            assert target_imagesize[0] % 64 == target_imagesize[1] % 64 == 0
            target_imagesize = np.array(target_imagesize).astype(int)
        
        target_latentsize = target_imagesize // 8

        if mask is None:
            mask = torch.ones([1, 1, target_imagesize[0], target_imagesize[1]], device=self.device, dtype=input_images.dtype)
        
        mask_ds = torch.nn.functional.interpolate(
            mask,
            size = [target_latentsize[0], target_latentsize[1]],
            mode = 'bilinear'
        )
        mask_ds[mask_ds > 0.0] = 1.0

        with torch.autocast(device_type='cuda'), self.model.ema_scope():
            if all(target_imagesize == input_images.size()[2:]): 
                input_upscaled = input_images.clone()
            else:
                if interpolation == 'esrgan':
                    input_upscaled = self.realesrgan_upsample(input_images, target_imagesize)
                    if out_fp is not None:
                        display_or_save(tensor_to_pil(input_upscaled), out_fp, esrgan_log_name )
                elif interpolation == 'sharpen':
                    input_upscaled = torch.nn.functional.interpolate(
                                            input_images,
                                            size = [target_imagesize[0], target_imagesize[1]],
                                            mode='bilinear',
                                            antialias=True,
                                        )
                    input_upscaled = sharpen(input_upscaled)
                else:                
                    input_upscaled = torch.nn.functional.interpolate(
                                            input_images,
                                            size = [target_imagesize[0], target_imagesize[1]],
                                            mode=interpolation,
                                            antialias=True,
                                        )
            if reference_image is not None:
                # replace the unmasked part of the input image with the reference image
                # according to mode, this reference is either lowpassed or not
                if not lowpass_reference == 'no':
                    if lowpass_reference == 'matching':
                        lp_target = [image_size_in[0], image_size_in[1]]
                    elif lowpass_reference == 'half':
                        lp_target = [target_imagesize[0] // 2, target_imagesize[1] // 2]
                    else:
                        raise ValueError('lowpass_reference should be "half" "matching" or "no"')
                    reference_image = torch.nn.functional.interpolate(
                                                    reference_image,
                                                    size = lp_target,
                                                    mode='bilinear',
                                                    antialias=True,
                    )
                    reference_image = torch.nn.functional.interpolate(
                                                    reference_image,
                                                    size = [target_imagesize[0], target_imagesize[1]],
                                                    mode='bilinear',
                                                    antialias=True,
                    )
                    logger.info(f'Downscaling to {image_size_in}, upscaling to {target_imagesize}.')
                np.testing.assert_array_equal(reference_image.shape[2:], target_imagesize)
                input_upscaled = mask * input_upscaled + (1 - mask) * reference_image


            latents_upscaled = self.model.scale_factor * \
                                    self.model.encode_first_stage(input_upscaled).mean

            uncond_conditioning = self.model.get_learned_conditioning(n_samples * [""])
            conditioning = self.model.get_learned_conditioning(n_samples * [prompt])
            
            np.testing.assert_array_equal(latents_upscaled.size()[2:], target_latentsize)
            np.testing.assert_array_equal(latents_upscaled.size()[2:], mask_ds.size()[2:])

            samples_upscaled, intermediates = self.sampler.blended_diffusion_sampling(
                source_img=latents_upscaled,
                mask=mask_ds,
                num_ddim_steps=ddim_steps,
                conditioning=conditioning,
                batch_size=n_samples,
                shape=[4, target_latentsize[0], target_latentsize[1]],
                verbose=False,
                unconditional_guidance_scale=7.5 if self.is_stable_diffusion else 5.0,
                unconditional_conditioning=uncond_conditioning,
                eta=ddim_eta,
                dilate_mask=dilate_mask,
                repaint_steps=repaint_steps,
                repaint_jump=repaint_jump,
                start_timestep=start_timestep,
            )


            image_samples_upscaled = self.model.decode_first_stage(samples_upscaled)
            image_samples_upscaled = torch.clamp((image_samples_upscaled + 1.0) / 2.0, min=0.0, max=1.0)

        
        return samples_upscaled, image_samples_upscaled, intermediates

    @classmethod
    def image_mask_from_pil(cls, image: Image, mask: Image) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        This function converts an image and mask, assuming they are both given as PIL,
        in modes "RGB" and "L" to the format used in LDM, i.e. scales the image to float on [-1.0,1.0] and 
        discretizes the mask to {0.0, 1.0}.
        """
        assert image.mode == 'RGB', 'Only input RGB-mode images please'
        assert mask.mode == 'L', 'Only input L-mode masks please'
        image = np.array(image)
        image = image.astype(np.float32)/255.0
        image = image[None].transpose(0, 3, 1, 2)
        image = torch.from_numpy(image)
        image = image * 2.0 - 1.0

        mask_pil = mask
        mask = np.array(mask_pil)
        mask = mask.astype(np.float32)/255.0
        mask = mask[None, None]
        mask[mask < 0.5] = 0.0
        mask[mask >= 0.5] = 1.0
        mask = torch.from_numpy(mask)

        return image, mask

    @classmethod
    def crop_image_and_mask(
        cls, 
        image: torch.Tensor, 
        mask: torch.Tensor, 
        crop_height: int, 
        crop_width: int
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[int], List[int]]:
        """
        Crops the image and mask to size `crop_height x crop_width` and returns the cropped image,
        cropped mask, a downsampled cropped masked and the crop's coordinates.

        Crops around the center of mass of the mask if this manages to include the whole mask, or if it is
        impossible to include the whole mask anyway.
        If it is possible to crop around the whole mask, but not by cropping around the center of mass, 
        we instead crop centered on the bounding box of the mask.
        """
        # x is "height" , y is "width"
        # the image representation is [batch, channel, x "height", y "width"]
        
        assert crop_height <= image.size()[2], 'can not fit crop into image, maybe the selected margin is too large?'
        assert crop_width <= image.size()[3], 'can not fit crop into image, maybe the selected margin is too large?'

        mask = mask.numpy()
        mask_center = np.mean(np.nonzero(mask)[2:4], 1).astype(np.int32)
        crop_x, crop_y = cls.get_crop_coords(image.size(), mask_center, crop_height, crop_width)

        # check if crop covers whole mask
        crop_filter = np.ones_like(mask, dtype=bool)
        crop_filter[:, :, crop_x[0]:crop_x[1], crop_y[0]:crop_y[1]] = False
        if np.sum(mask[crop_filter]):  # does not cover full mask
            # check if bounding box would work, if yes use bounding box instead of center of mass
            bbox_x = [
                np.min(np.nonzero(mask)[2]).astype(np.int32),
                np.max(np.nonzero(mask)[2]).astype(np.int32)
            ]
            bbox_y = [
                np.min(np.nonzero(mask)[3]).astype(np.int32),
                np.max(np.nonzero(mask)[3]).astype(np.int32)
            ]
            if bbox_x[1] - bbox_x[0] < crop_height and bbox_y[1] - bbox_y[0] < crop_width:
                mask_center = np.mean([bbox_x, bbox_y], 1).astype(np.int32)
                crop_x, crop_y = cls.get_crop_coords(image.size(), mask_center, crop_height, crop_width)
        
        crop_filter = np.ones_like(mask, dtype=bool)
        crop_filter[:, :, crop_x[0]:crop_x[1], crop_y[0]:crop_y[1]] = False
        if np.sum(mask[crop_filter]):  # does not cover full mask
            logger.warn('Crop is not covering full mask!')


        img_crop = image[:, :, crop_x[0]:crop_x[1], crop_y[0]:crop_y[1]]
        mask_crop = mask[:, :, crop_x[0]:crop_x[1], crop_y[0]:crop_y[1]]



        assert img_crop.shape[2] == crop_height  # sanity check
        assert img_crop.shape[3] == crop_width

        mask_crop = torch.from_numpy(mask_crop)
        mask_crop_ds = torch.nn.functional.interpolate(
            mask_crop, size=[crop_height // 8, crop_width // 8], mode='bilinear', align_corners=False
        )
        mask_crop_ds[mask_crop_ds > 0.0] = 1.0

        return img_crop, mask_crop, mask_crop_ds, crop_x, crop_y

    @classmethod
    def get_crop_coords(cls, image_size: np.ndarray, mask_center: np.ndarray, crop_height: int, crop_width: int):
        """
        Given an image size, crop dimensions and center this method returns the coordinates of a rectangle
        that does not exceed the boundaries of the image and is as closeas possible to the desired center.
        """
        # np.floor/ np.ceil are necessary to ensure a correct sized crop in case of odd sizes
        if mask_center[0] - np.floor(crop_height / 2) < 0:  # check if area would be out of bounds
            crop_x = [0, crop_height]
        elif mask_center[0] + np.ceil(crop_height / 2) > image_size[2]:  # check if area would be out of bounds
            crop_x = [image_size[2] - crop_height, image_size[2]]
        else:
            crop_x = [mask_center[0] - np.floor(crop_height / 2), mask_center[0] + np.ceil(crop_height / 2)]

        if mask_center[1] - np.floor(crop_width / 2) < 0:
            crop_y = [0, crop_width]
        elif mask_center[1] + np.ceil(crop_width / 2) > image_size[3]:
            crop_y = [image_size[3] - crop_width, image_size[3]]
        else:
            crop_y = [mask_center[1] - np.floor(crop_width / 2), mask_center[1] + np.ceil(crop_width / 2)]
        crop_x = [int(crop_x[0]), int(crop_x[1])]
        crop_y = [int(crop_y[0]), int(crop_y[1])]
        return crop_x, crop_y


    @torch.enable_grad()
    def _decoder_optimize_sample_batch(
        self,
        input_latents: torch.Tensor,
        reference_imgs: torch.Tensor,
        masks: torch.Tensor,
        n_iterations: int,
        background_preserve_weight: float,
        optim_partial: torch.optim.Optimizer,
        batch_size: int = 2,
        overlap_corrections: torch.Tensor = None,
    ) -> torch.Tensor:
        """
        Internal function for decoder optimization. 

        If more than one input is given this runs the decoder optimization across all given samples in an SGD like way.
        """
        start_time = time.time()
        assert len(input_latents.shape) == 4 , 'input batch of samples'
        assert input_latents.shape[0] == input_latents.shape[0] == masks.shape[0], 'number of reference images, masks and latents must match'
            
        original_decoder_outs = []
        for latent in input_latents:
            original_decoder_outs.append(self.model.differentiable_decode_first_stage(latent[None]).detach())
        original_decoder_outs = torch.vstack(original_decoder_outs)

        if overlap_corrections is None:
            overlap_corrections = torch.ones_like(original_decoder_outs)
        
        optim = optim_partial(self.model.first_stage_model.decoder.parameters())
        optim.zero_grad()
        logger.info(f'Starting decoder optimization for {n_iterations} iterations.')
        for decoder_opt_it in range(n_iterations):
            loss_sum = 0.0
            n_batch = int(np.ceil(input_latents.shape[0] / batch_size))
            for idx in range(n_batch):
                latent = input_latents[idx * batch_size: (idx + 1) * batch_size]
                mask = masks[idx * batch_size: (idx + 1) * batch_size]
                ref = reference_imgs[idx * batch_size: (idx + 1) * batch_size]
                overlap_correction = overlap_corrections[idx * batch_size: (idx + 1) * batch_size]
                original_decoder_out = original_decoder_outs[idx * batch_size: (idx + 1) * batch_size]

                current_decoder_out = self.model.differentiable_decode_first_stage(latent)
                fg_loss = (mask * (current_decoder_out - original_decoder_out).square() * overlap_correction).mean()
                bg_loss = ((1 - mask) * (current_decoder_out - ref).square() * overlap_correction).mean()
                loss = fg_loss + background_preserve_weight * bg_loss
                loss.backward()
                loss_sum += loss.detach().cpu().numpy()
                optim.step()
                optim.zero_grad()
            logger.info(
                f'Decoder opt it {decoder_opt_it}/{n_iterations} loss: {loss_sum}')
        logger.info(f'Finished decoder optimization in {time.time() - start_time:.2f} seconds.')

        with torch.no_grad():
            optimized_samples = []
            for latent in input_latents:
                optimized_samples.append(self.model.differentiable_decode_first_stage(latent[None]))
        
        return torch.vstack(optimized_samples)


    def decoder_optimization(
        self,
        input_sample: torch.Tensor,
        reference_img: torch.Tensor,
        mask: torch.Tensor,
        n_decoder_opt_it: int = 75,
        background_preserve_weight: float = 100.0,
        learn_rate: float = 1e-4,
        reuse_last_cp: bool = False,
        max_edgelen: int = None,  # after this size we split into segments
        batch_size: int = 2,  # only used when doing optimization in segments.
    ) -> torch.Tensor:
        """
        Performs decoder optimization, finetuning the decoder weights to minimize the MSE between the decoded sample and the unmasked area,
        while maintaining the model output in the masked area.

        If the input image is larger than the maximum resolution of the diffusion model (self.max_edgelen), the image is split into a grid,
        each image is considered a batch and we run `n_decoder_opt_it` epochs of decoder finetuning.
        """
        if max_edgelen is None:
            max_edgelen = self.max_edgelen
        if self.model.dtype == torch.float16:
            self.model.first_stage_model.float()

        logger.info('Starting decoder optimization')

        if input_sample.min() >= 0.0:
            logger.warning(f'This function should receive images scaled to [-1.0, 1.0], not [0.0, 1.0]')
        assert len(input_sample.shape) == 4 and input_sample.shape[0] == 1, 'Insert images as a batch of size one, i.e. [1, C, H , W]'


        if input_sample.shape[2] > max_edgelen or input_sample.shape[3] > max_edgelen:  
            # this requirement is a bit too strict, we could probably fit larger images into the decoder optimization
            # by moving the unet and encoder back to cpu before optimization,
            # but at some point we would hit a limit anyway.
            
            x_crops, y_crops = self.determine_upsampling_split(target_image_size=input_sample.shape[2:], max_edgelen=self.default_size, 
                                                            overlap=0, enforce_min_size=True)
            logger.info(f'Performing decoder optimization in {len(x_crops)} segments.')
            input_latents = []
            reference_imgs = []
            masks = []
            overlap_count_matrix = torch.zeros(input_sample.shape[2:], device=input_sample.device)
            for x_crop, y_crop in zip(x_crops, y_crops):
                input_latents.append(
                    self.model.scale_factor * self.model.encode_first_stage(input_sample[:,:,x_crop[0]:x_crop[1], y_crop[0]:y_crop[1]]).mean
                )
                reference_imgs.append(reference_img[:, :, x_crop[0]:x_crop[1], y_crop[0]:y_crop[1]])
                masks.append(mask[:, :, x_crop[0]:x_crop[1], y_crop[0]:y_crop[1]])
                overlap_count_matrix[x_crop[0]:x_crop[1], y_crop[0]:y_crop[1]] = overlap_count_matrix[x_crop[0]:x_crop[1], y_crop[0]:y_crop[1]] + 1.0
            input_latents = torch.vstack(input_latents)
            reference_imgs = torch.vstack(reference_imgs)
            masks = torch.vstack(masks)

            overlap_correction_matrix = 1 / overlap_count_matrix
            overlap_corrections = []
            for x_crop, y_crop in zip(x_crops, y_crops):
                overlap_corrections.append(overlap_correction_matrix[None, None, x_crop[0]:x_crop[1], y_crop[0]:y_crop[1]])
            overlap_corrections = torch.vstack(overlap_corrections)
        else:
            input_latents = self.model.scale_factor * self.model.encode_first_stage(input_sample).mean
            reference_imgs = reference_img
            masks = mask
            overlap_corrections = None

        original_decoder_state_dict = copy.deepcopy(
            self.model.first_stage_model.decoder.state_dict())

        if reuse_last_cp and self.last_decoderopt_dict is not None:
            logger.info('resuming decoder optimization from last checkpoint')
            self.model.first_stage_model.decoder.load_state_dict(
                self.last_decoderopt_dict
            )

        for param in self.model.first_stage_model.decoder.parameters():
            param.requires_grad = True

        decode_optimizer = partial(torch.optim.Adam, lr=learn_rate)
        try:
            optimized_samples = self._decoder_optimize_sample_batch(
                input_latents,
                reference_imgs,
                masks,
                n_decoder_opt_it,
                background_preserve_weight,
                optim_partial=decode_optimizer,
                overlap_corrections=overlap_corrections,
                batch_size=batch_size
            )
        except KeyboardInterrupt:  # prevents issues with notebooks
            self.model.first_stage_model.decoder.load_state_dict(
                original_decoder_state_dict)
            raise

        if input_sample.shape[2] > self.max_edgelen or input_sample.shape[3] > self.max_edgelen:  
            # decode the segments of the image with the new decoder weights and blend them as we do for the `upscale_in_parts`
            # just use upscale_in_parts with the target size being the current size and start_timestep=0.0
            _, optimized_sample = self.upscale_in_parts(
                input_image = input_sample,
                prompt='',
                upscale_factor=1.0,
                start_timestep=0.0,
            )

        else:
            optimized_sample = optimized_samples.detach()
            optimized_sample = torch.clamp(
                (optimized_sample.detach() + 1.0) / 2.0, min=0.0, max=1.0)


        self.last_decoderopt_dict = self.model.first_stage_model.decoder.state_dict()
        self.model.first_stage_model.decoder.load_state_dict(
            original_decoder_state_dict)

        if self.model.dtype == torch.float16:
            self.model.first_stage_model.half()

        return optimized_sample


